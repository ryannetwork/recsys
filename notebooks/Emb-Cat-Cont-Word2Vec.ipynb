{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from numpy import vstack\n",
    "from numpy import argmax\n",
    "from pandas import read_csv\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch import Tensor\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from torch.nn import Linear\n",
    "from torch.nn import ReLU\n",
    "from torch.nn import Softmax\n",
    "from torch.nn import Module\n",
    "from torch.optim import SGD\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.nn.init import kaiming_uniform_\n",
    "from torch.nn.init import xavier_uniform_\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as torch_optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/korniltsevdmitry/Desktop/python/projects/recommendation_systems/notebooks'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/result_df.csv', nrows=2000)\n",
    "# data = pd.read_csv('../data/result_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hash_inn</th>\n",
       "      <th>okved2</th>\n",
       "      <th>region</th>\n",
       "      <th>0_count_kt</th>\n",
       "      <th>1_count_kt</th>\n",
       "      <th>10_count_kt</th>\n",
       "      <th>1000_count_kt</th>\n",
       "      <th>11_count_kt</th>\n",
       "      <th>12_count_kt</th>\n",
       "      <th>13_count_kt</th>\n",
       "      <th>...</th>\n",
       "      <th>share_72_dt_dt</th>\n",
       "      <th>share_73_dt_dt</th>\n",
       "      <th>share_74_dt_dt</th>\n",
       "      <th>share_75_dt_dt</th>\n",
       "      <th>share_76_dt_dt</th>\n",
       "      <th>share_77_dt_dt</th>\n",
       "      <th>share_78_dt_dt</th>\n",
       "      <th>share_79_dt_dt</th>\n",
       "      <th>share_8_dt_dt</th>\n",
       "      <th>share_9_dt_dt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>61058</td>\n",
       "      <td>34</td>\n",
       "      <td>86</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8311</td>\n",
       "      <td>18</td>\n",
       "      <td>86</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 331 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   hash_inn  okved2  region  0_count_kt  1_count_kt  10_count_kt  \\\n",
       "0     61058      34      86         NaN         NaN          NaN   \n",
       "1      8311      18      86         NaN         NaN          NaN   \n",
       "\n",
       "   1000_count_kt  11_count_kt  12_count_kt  13_count_kt  ...  share_72_dt_dt  \\\n",
       "0            NaN          NaN          NaN          NaN  ...             NaN   \n",
       "1            NaN          NaN          NaN          NaN  ...             NaN   \n",
       "\n",
       "   share_73_dt_dt  share_74_dt_dt  share_75_dt_dt  share_76_dt_dt  \\\n",
       "0             NaN             NaN             NaN             NaN   \n",
       "1             NaN             NaN             NaN             NaN   \n",
       "\n",
       "   share_77_dt_dt  share_78_dt_dt  share_79_dt_dt  share_8_dt_dt  \\\n",
       "0             NaN             NaN             NaN            NaN   \n",
       "1             NaN             NaN             NaN            NaN   \n",
       "\n",
       "   share_9_dt_dt  \n",
       "0            NaN  \n",
       "1            NaN  \n",
       "\n",
       "[2 rows x 331 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add text data to create word2vec embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    'he is a king',\n",
    "    'she is a queen',\n",
    "    'he is a man',\n",
    "    'she is a woman',\n",
    "    'warsaw is poland capital',\n",
    "    'berlin is germany capital',\n",
    "    'paris is france capital',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_corpus(corpus):\n",
    "    tokens = [x.split() for x in corpus]\n",
    "    return tokens\n",
    "\n",
    "tokenized_corpus = tokenize_corpus(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['he', 'is', 'a', 'king'],\n",
       " ['she', 'is', 'a', 'queen'],\n",
       " ['he', 'is', 'a', 'man'],\n",
       " ['she', 'is', 'a', 'woman'],\n",
       " ['warsaw', 'is', 'poland', 'capital'],\n",
       " ['berlin', 'is', 'germany', 'capital'],\n",
       " ['paris', 'is', 'france', 'capital']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_col = pd.DataFrame(corpus*300).iloc[0:200,]\n",
    "text_col.columns = ['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_col['tokens'] = text_col['text'].apply(lambda x: tokenize_corpus(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>he is a king</td>\n",
       "      <td>[[h], [e], [], [i], [s], [], [a], [], [k], [i]...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>she is a queen</td>\n",
       "      <td>[[s], [h], [e], [], [i], [s], [], [a], [], [q]...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             text                                             tokens\n",
       "0    he is a king  [[h], [e], [], [i], [s], [], [a], [], [k], [i]...\n",
       "1  she is a queen  [[s], [h], [e], [], [i], [s], [], [a], [], [q]..."
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_col.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = []\n",
    "for sentence in tokenized_corpus:\n",
    "    for token in sentence:\n",
    "        if token not in vocabulary:\n",
    "            vocabulary.append(token)\n",
    "\n",
    "word2idx = {w: idx for (idx, w) in enumerate(vocabulary)}\n",
    "idx2word = {idx: w for (idx, w) in enumerate(vocabulary)}\n",
    "\n",
    "vocabulary_size = len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 2\n",
    "idx_pairs = []\n",
    "# for each sentence\n",
    "for sentence in tokenized_corpus:\n",
    "    indices = [word2idx[word] for word in sentence]\n",
    "    # for each word, threated as center word\n",
    "    for center_word_pos in range(len(indices)):\n",
    "        # for each window position\n",
    "        for w in range(-window_size, window_size + 1):\n",
    "            context_word_pos = center_word_pos + w\n",
    "            # make soure not jump out sentence\n",
    "            if context_word_pos < 0 or context_word_pos >= len(indices) or center_word_pos == context_word_pos:\n",
    "                continue\n",
    "            context_word_idx = indices[context_word_pos]\n",
    "            idx_pairs.append((indices[center_word_pos], context_word_idx))\n",
    "\n",
    "idx_pairs = np.array(idx_pairs) # it will be useful to have this as numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### determ categorical and continious columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_col = [\"region\"]\n",
    "cont_col = [col for col in data.columns if 'share' in col]\n",
    "target_col = [\"okved2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[cont_col + cat_col + target_col]\n",
    "data.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data.okved2.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>share_0_kt</th>\n",
       "      <th>share_1_kt</th>\n",
       "      <th>share_10_kt</th>\n",
       "      <th>share_1000_kt</th>\n",
       "      <th>share_11_kt</th>\n",
       "      <th>share_12_kt</th>\n",
       "      <th>share_13_kt</th>\n",
       "      <th>share_14_kt</th>\n",
       "      <th>share_15_kt</th>\n",
       "      <th>share_16_kt</th>\n",
       "      <th>...</th>\n",
       "      <th>share_74_dt_dt</th>\n",
       "      <th>share_75_dt_dt</th>\n",
       "      <th>share_76_dt_dt</th>\n",
       "      <th>share_77_dt_dt</th>\n",
       "      <th>share_78_dt_dt</th>\n",
       "      <th>share_79_dt_dt</th>\n",
       "      <th>share_8_dt_dt</th>\n",
       "      <th>share_9_dt_dt</th>\n",
       "      <th>region</th>\n",
       "      <th>okved2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>86</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>86</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 164 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   share_0_kt  share_1_kt  share_10_kt  share_1000_kt  share_11_kt  \\\n",
       "0         0.0         0.0          0.0            0.0          0.0   \n",
       "1         0.0         0.0          0.0            0.0          0.0   \n",
       "\n",
       "   share_12_kt  share_13_kt  share_14_kt  share_15_kt  share_16_kt  ...  \\\n",
       "0          0.0          0.0          0.0          0.0          0.0  ...   \n",
       "1          0.0          0.0          0.0          0.0          0.0  ...   \n",
       "\n",
       "   share_74_dt_dt  share_75_dt_dt  share_76_dt_dt  share_77_dt_dt  \\\n",
       "0             0.0             0.0             0.0             0.0   \n",
       "1             0.0             0.0             0.0             0.0   \n",
       "\n",
       "   share_78_dt_dt  share_79_dt_dt  share_8_dt_dt  share_9_dt_dt  region  \\\n",
       "0             0.0             0.0            0.0            0.0      86   \n",
       "1             0.0             0.0            0.0            0.0      86   \n",
       "\n",
       "   okved2  \n",
       "0      34  \n",
       "1      18  \n",
       "\n",
       "[2 rows x 164 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoders = {}\n",
    "for col in cat_col:\n",
    "    label_encoders[col] = LabelEncoder()\n",
    "    data[col] = label_encoders[col].fit_transform(data[col]) \n",
    "    data[col] = data[col].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/preprocessing/_label.py:251: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "Y = LabelEncoder().fit_transform(data[target_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[cont_col + cat_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_cols_dict = {n: len(col.cat.categories) for n, col in X[cat_col].items() if len(col.cat.categories) > 2}\n",
    "emb_cols = emb_cols_dict.keys()\n",
    "emb_sizes = [(c, min(50, (c+1)//2)) for _,c in emb_cols_dict.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(77, 39)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>share_0_kt</th>\n",
       "      <th>share_1_kt</th>\n",
       "      <th>share_10_kt</th>\n",
       "      <th>share_1000_kt</th>\n",
       "      <th>share_11_kt</th>\n",
       "      <th>share_12_kt</th>\n",
       "      <th>share_13_kt</th>\n",
       "      <th>share_14_kt</th>\n",
       "      <th>share_15_kt</th>\n",
       "      <th>share_16_kt</th>\n",
       "      <th>...</th>\n",
       "      <th>share_73_dt_dt</th>\n",
       "      <th>share_74_dt_dt</th>\n",
       "      <th>share_75_dt_dt</th>\n",
       "      <th>share_76_dt_dt</th>\n",
       "      <th>share_77_dt_dt</th>\n",
       "      <th>share_78_dt_dt</th>\n",
       "      <th>share_79_dt_dt</th>\n",
       "      <th>share_8_dt_dt</th>\n",
       "      <th>share_9_dt_dt</th>\n",
       "      <th>region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>915</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 163 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     share_0_kt  share_1_kt  share_10_kt  share_1000_kt  share_11_kt  \\\n",
       "81          0.0         0.0          0.0       0.285714          0.0   \n",
       "915         0.0         0.0          0.0       0.000000          0.0   \n",
       "\n",
       "     share_12_kt  share_13_kt  share_14_kt  share_15_kt  share_16_kt  ...  \\\n",
       "81           0.0          0.0          0.0          0.0          0.0  ...   \n",
       "915          0.0          0.0          0.0          0.0          0.0  ...   \n",
       "\n",
       "     share_73_dt_dt  share_74_dt_dt  share_75_dt_dt  share_76_dt_dt  \\\n",
       "81              0.0             0.0             0.0             0.0   \n",
       "915             0.0             0.0             0.0             0.0   \n",
       "\n",
       "     share_77_dt_dt  share_78_dt_dt  share_79_dt_dt  share_8_dt_dt  \\\n",
       "81              0.0             0.0             0.0            0.0   \n",
       "915             0.0             0.0             0.0            0.0   \n",
       "\n",
       "     share_9_dt_dt  region  \n",
       "81             0.0      71  \n",
       "915            0.0      71  \n",
       "\n",
       "[2 rows x 163 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, Y, test_size=0.33, random_state=42)\n",
    "X_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShelterOutcomeDataset(Dataset):\n",
    "    def __init__(self, X, Y, emb_cols):\n",
    "        X = X.copy()\n",
    "        #categorical columns\n",
    "        self.X1 = X.loc[:, emb_cols].copy().values.astype(np.int64)\n",
    "        #numerical columns\n",
    "        self.X2 = X.drop(columns=emb_cols).copy().values.astype(np.float32)\n",
    "        self.y = Y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X1[idx], self.X2[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating train and valid datasets\n",
    "train_ds = ShelterOutcomeDataset(X_train, y_train, emb_cols)\n",
    "valid_ds = ShelterOutcomeDataset(X_val, y_val, emb_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShelterOutcomeModel(nn.Module):\n",
    "    def __init__(self, embedding_sizes, n_cont, target_dim):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.ModuleList([nn.Embedding(categories, size) for categories,size in embedding_sizes])\n",
    "        n_emb = sum(e.embedding_dim for e in self.embeddings) #length of all embeddings combined\n",
    "        self.n_emb = n_emb\n",
    "        self.n_cont = n_cont\n",
    "        self.target_dim = target_dim\n",
    "        self.lin1 = nn.Linear(self.n_emb + self.n_cont, 800)\n",
    "        self.lin2 = nn.Linear(800, 280)\n",
    "        self.lin3 = nn.Linear(280, self.target_dim)\n",
    "        self.bn1 = nn.BatchNorm1d(self.n_cont)\n",
    "        self.bn2 = nn.BatchNorm1d(800)\n",
    "        self.bn3 = nn.BatchNorm1d(280)\n",
    "        self.emb_drop = nn.Dropout(0.6)\n",
    "        self.drops = nn.Dropout(0.3)\n",
    "        \n",
    "\n",
    "    def forward(self, x_cat, x_cont):\n",
    "        x = [e(x_cat[:,i]) for i,e in enumerate(self.embeddings)]\n",
    "        x = torch.cat(x, 1)\n",
    "        x = self.emb_drop(x)\n",
    "        x2 = self.bn1(x_cont)\n",
    "        x = torch.cat([x, x2], 1)\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = self.drops(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(self.lin2(x))\n",
    "        x = self.drops(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.lin3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "162"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cont_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pd.Series(Y).unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ShelterOutcomeModel(\n",
       "  (embeddings): ModuleList(\n",
       "    (0): Embedding(77, 39)\n",
       "  )\n",
       "  (lin1): Linear(in_features=201, out_features=800, bias=True)\n",
       "  (lin2): Linear(in_features=800, out_features=280, bias=True)\n",
       "  (lin3): Linear(in_features=280, out_features=75, bias=True)\n",
       "  (bn1): BatchNorm1d(162, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn2): BatchNorm1d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn3): BatchNorm1d(280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (emb_drop): Dropout(p=0.6, inplace=False)\n",
       "  (drops): Dropout(p=0.3, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ShelterOutcomeModel(embedding_sizes=emb_sizes, n_cont=len(cont_col), target_dim = len(pd.Series(Y).unique()))\n",
    "to_device(model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(model, lr = 0.001, wd = 0.0):\n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    optim = torch_optim.Adam(parameters, lr=lr, weight_decay=wd)\n",
    "    return optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optim, train_dl):\n",
    "    model.train()\n",
    "    total = 0\n",
    "    sum_loss = 0\n",
    "    for x1, x2, y in train_dl:\n",
    "        batch = y.shape[0]\n",
    "        output = model(x1, x2)\n",
    "        loss = F.cross_entropy(output, y)   \n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        total += batch\n",
    "        sum_loss += batch*(loss.item())\n",
    "        \n",
    "        print('weights')\n",
    "        print(torch.sum(model.lin3.weight.data))\n",
    "        trained_weights = model.lin3.weight.data\n",
    "#     print(trained_weights)\n",
    "    return sum_loss/total, trained_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import copy\n",
    "# init_weights = copy.deepcopy(model.fc1.weight.data)\n",
    "# for epoch in range(1, 3):\n",
    "#     for batch_idx, (dat, target) in enumerate(train_loader):\n",
    "#         data, target = Variable(dat), Variable(target).detach()\n",
    "#         optimizer.zero_grad()\n",
    "#         output = model(data)\n",
    "#         loss = criterion(output, target)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         print(torch.sum(model.fc1.weight.data))\n",
    "# trained_weights = model.fc1.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m = nn.Embedding(...)\n",
    "# # m.weight contains the embedding weights.\n",
    "# parameters = m.weight.numpy()\n",
    "# # save it as you wish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_loss(model, valid_dl):\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    sum_loss = 0\n",
    "    correct = 0\n",
    "    predictions, actuals = list(), list()\n",
    "    for x1, x2, y in valid_dl:\n",
    "        current_batch_size = y.shape[0]\n",
    "        out = model(x1, x2)\n",
    "        loss = F.cross_entropy(out, y)\n",
    "        sum_loss += current_batch_size*(loss.item())\n",
    "        total += current_batch_size\n",
    "        pred = torch.max(out, 1)[1]\n",
    "        correct += (pred == y).float().sum().item()\n",
    "        \n",
    "        y = y.reshape((len(y), 1))\n",
    "        pred = pred.reshape((len(pred), 1))\n",
    "        # store\n",
    "        predictions.append(pred)\n",
    "        actuals.append(y)\n",
    "    print(len(predictions), len(actuals))\n",
    "    print(\"valid loss %.3f and accuracy %.3f\" % (sum_loss/total, correct/total))\n",
    "    predictions, actuals = vstack(predictions), vstack(actuals)\n",
    "    return actuals, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model, epochs, lr=0.01, wd=0.0):\n",
    "    optim = get_optimizer(model, lr = lr, wd = wd)\n",
    "    for i in range(epochs): \n",
    "        loss, weights = train_model(model, optim, train_dl)\n",
    "        print(\"training loss: \", loss)\n",
    "        actuals, predictions = val_loss(model, valid_dl)\n",
    "    return actuals, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=256\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights\n",
      "tensor(-4138.0127)\n",
      "weights\n",
      "tensor(-4238.5894)\n",
      "weights\n",
      "tensor(-4312.7754)\n",
      "weights\n",
      "tensor(-4382.9590)\n",
      "weights\n",
      "tensor(-4443.7563)\n",
      "weights\n",
      "tensor(-4504.9312)\n",
      "training loss:  2.0992072959444417\n",
      "3 3\n",
      "valid loss 5.630 and accuracy 0.303\n",
      "weights\n",
      "tensor(-4551.7847)\n",
      "weights\n",
      "tensor(-4592.7144)\n",
      "weights\n",
      "tensor(-4632.6152)\n",
      "weights\n",
      "tensor(-4671.5112)\n",
      "weights\n",
      "tensor(-4713.1274)\n",
      "weights\n",
      "tensor(-4760.2236)\n",
      "training loss:  1.9940433651653688\n",
      "3 3\n",
      "valid loss 5.804 and accuracy 0.238\n",
      "weights\n",
      "tensor(-4804.0005)\n",
      "weights\n",
      "tensor(-4848.6763)\n",
      "weights\n",
      "tensor(-4890.6982)\n",
      "weights\n",
      "tensor(-4927.4727)\n",
      "weights\n",
      "tensor(-4959.8823)\n",
      "weights\n",
      "tensor(-4994.0210)\n",
      "training loss:  1.8931541371701368\n",
      "3 3\n",
      "valid loss 5.722 and accuracy 0.241\n",
      "weights\n",
      "tensor(-5029.1479)\n",
      "weights\n",
      "tensor(-5067.5918)\n",
      "weights\n",
      "tensor(-5106.9971)\n",
      "weights\n",
      "tensor(-5144.8721)\n",
      "weights\n",
      "tensor(-5180.1641)\n",
      "weights\n",
      "tensor(-5216.9868)\n",
      "training loss:  1.7992042972080744\n",
      "3 3\n",
      "valid loss 5.742 and accuracy 0.282\n",
      "weights\n",
      "tensor(-5251.4819)\n",
      "weights\n",
      "tensor(-5283.0698)\n",
      "weights\n",
      "tensor(-5313.5781)\n",
      "weights\n",
      "tensor(-5343.5596)\n",
      "weights\n",
      "tensor(-5374.1548)\n",
      "weights\n",
      "tensor(-5410.1260)\n",
      "training loss:  1.6627954571994383\n",
      "3 3\n",
      "valid loss 6.120 and accuracy 0.247\n",
      "weights\n",
      "tensor(-5443.7354)\n",
      "weights\n",
      "tensor(-5474.4663)\n",
      "weights\n",
      "tensor(-5502.9067)\n",
      "weights\n",
      "tensor(-5526.3628)\n",
      "weights\n",
      "tensor(-5548.4199)\n",
      "weights\n",
      "tensor(-5573.8633)\n",
      "training loss:  1.5996204753420247\n",
      "3 3\n",
      "valid loss 6.616 and accuracy 0.209\n",
      "weights\n",
      "tensor(-5598.7080)\n",
      "weights\n",
      "tensor(-5624.0996)\n",
      "weights\n",
      "tensor(-5648.9805)\n",
      "weights\n",
      "tensor(-5670.7632)\n",
      "weights\n",
      "tensor(-5688.9805)\n",
      "weights\n",
      "tensor(-5708.1914)\n",
      "training loss:  1.5710350047296553\n",
      "3 3\n",
      "valid loss 6.700 and accuracy 0.218\n",
      "weights\n",
      "tensor(-5725.3545)\n",
      "weights\n",
      "tensor(-5741.8145)\n",
      "weights\n",
      "tensor(-5758.6406)\n",
      "weights\n",
      "tensor(-5775.6768)\n",
      "weights\n",
      "tensor(-5791.5532)\n",
      "weights\n",
      "tensor(-5808.9224)\n",
      "training loss:  1.5061250925064087\n",
      "3 3\n",
      "valid loss 6.792 and accuracy 0.209\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         2\n",
      "           2       0.00      0.00      0.00         4\n",
      "           3       0.27      0.36      0.31        33\n",
      "           4       0.00      0.00      0.00         2\n",
      "           5       0.12      0.11      0.12         9\n",
      "           6       0.00      0.00      0.00         1\n",
      "           7       0.08      0.04      0.06        23\n",
      "           8       0.00      0.00      0.00         7\n",
      "           9       0.00      0.00      0.00         3\n",
      "          10       0.20      0.35      0.26        17\n",
      "          11       0.37      0.40      0.38       188\n",
      "          12       0.31      0.31      0.31        35\n",
      "          13       0.00      0.00      0.00         1\n",
      "          14       0.00      0.00      0.00         4\n",
      "          15       0.00      0.00      0.00         1\n",
      "          16       0.00      0.00      0.00         7\n",
      "          17       0.00      0.00      0.00         2\n",
      "          18       0.00      0.00      0.00         1\n",
      "          19       0.31      0.44      0.36         9\n",
      "          20       0.50      0.12      0.20         8\n",
      "          21       0.00      0.00      0.00         6\n",
      "          22       0.00      0.00      0.00         1\n",
      "          23       0.00      0.00      0.00         1\n",
      "          24       0.30      0.21      0.25        14\n",
      "          25       0.00      0.00      0.00         1\n",
      "          27       0.00      0.00      0.00         7\n",
      "          28       0.00      0.00      0.00         6\n",
      "          29       0.00      0.00      0.00         1\n",
      "          30       0.00      0.00      0.00         5\n",
      "          31       0.00      0.00      0.00         2\n",
      "          32       0.14      0.24      0.18        51\n",
      "          36       0.00      0.00      0.00         3\n",
      "          37       0.00      0.00      0.00        14\n",
      "          39       0.00      0.00      0.00         3\n",
      "          40       0.00      0.00      0.00         2\n",
      "          41       0.00      0.00      0.00         5\n",
      "          42       0.00      0.00      0.00         3\n",
      "          44       0.08      0.08      0.08        12\n",
      "          45       0.00      0.00      0.00         5\n",
      "          46       0.00      0.00      0.00        10\n",
      "          47       0.00      0.00      0.00         5\n",
      "          48       0.00      0.00      0.00         1\n",
      "          49       0.23      0.27      0.25        26\n",
      "          50       0.40      0.50      0.44         4\n",
      "          51       0.00      0.00      0.00         3\n",
      "          52       0.00      0.00      0.00        20\n",
      "          53       0.00      0.00      0.00         8\n",
      "          54       0.00      0.00      0.00         1\n",
      "          55       0.00      0.00      0.00         2\n",
      "          56       0.00      0.00      0.00         6\n",
      "          57       0.00      0.00      0.00         6\n",
      "          58       0.00      0.00      0.00        12\n",
      "          59       0.07      0.09      0.08        11\n",
      "          60       0.00      0.00      0.00         1\n",
      "          61       0.00      0.00      0.00         4\n",
      "          62       0.00      0.00      0.00         4\n",
      "          63       0.00      0.00      0.00         0\n",
      "          64       0.00      0.00      0.00         4\n",
      "          65       0.00      0.00      0.00         2\n",
      "          67       0.00      0.00      0.00         2\n",
      "          68       0.00      0.00      0.00         5\n",
      "          69       0.17      0.10      0.12        10\n",
      "          70       0.00      0.00      0.00         0\n",
      "          71       0.00      0.00      0.00         5\n",
      "          72       0.00      0.00      0.00         3\n",
      "          73       0.00      0.00      0.00         2\n",
      "          74       0.00      0.00      0.00         4\n",
      "\n",
      "    accuracy                           0.21       660\n",
      "   macro avg       0.05      0.05      0.05       660\n",
      "weighted avg       0.19      0.21      0.20       660\n",
      "\n"
     ]
    }
   ],
   "source": [
    "actuals, predictions = train_loop(model, epochs=8, lr=0.05, wd=0.00001)\n",
    "print(classification_report(actuals, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss:  2.7150109985951065\n",
      "310 310\n",
      "valid loss 2.628 and accuracy 0.331\n",
      "training loss:  2.6515707370202426\n",
      "310 310\n",
      "valid loss 2.628 and accuracy 0.318\n",
      "training loss:  2.6611339436356714\n",
      "310 310\n",
      "valid loss 2.630 and accuracy 0.334\n",
      "training loss:  2.6687577738265693\n",
      "310 310\n",
      "valid loss 2.613 and accuracy 0.335\n",
      "training loss:  2.672064993795145\n",
      "310 310\n",
      "valid loss 2.655 and accuracy 0.329\n",
      "training loss:  2.679746832163435\n",
      "310 310\n",
      "valid loss 2.642 and accuracy 0.332\n",
      "training loss:  2.67826179908855\n",
      "310 310\n",
      "valid loss 2.676 and accuracy 0.304\n",
      "training loss:  2.67923294631206\n",
      "310 310\n",
      "valid loss 2.640 and accuracy 0.310\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.18      0.07      0.10       262\n",
      "           1       0.00      0.00      0.00        19\n",
      "           2       0.00      0.00      0.00        29\n",
      "           3       0.00      0.00      0.00       265\n",
      "           4       0.25      0.01      0.01      2552\n",
      "           5       0.00      0.00      0.00        75\n",
      "           6       0.67      0.04      0.08       809\n",
      "           7       0.00      0.00      0.00       214\n",
      "           8       0.44      0.01      0.02      1542\n",
      "           9       0.00      0.00      0.00       617\n",
      "          10       0.00      0.00      0.00       342\n",
      "          11       0.29      0.10      0.15       929\n",
      "          12       0.27      0.59      0.37     14693\n",
      "          13       0.00      0.00      0.00        25\n",
      "          14       0.45      0.25      0.32      3392\n",
      "          15       0.00      0.00      0.00       138\n",
      "          16       0.00      0.00      0.00       604\n",
      "          17       0.00      0.00      0.00        73\n",
      "          18       0.38      0.13      0.19       660\n",
      "          19       0.00      0.00      0.00       184\n",
      "          20       0.00      0.00      0.00       283\n",
      "          21       0.17      0.02      0.03       445\n",
      "          22       0.05      0.00      0.01       441\n",
      "          23       0.00      0.00      0.00       174\n",
      "          24       0.16      0.20      0.18       125\n",
      "          25       0.00      0.00      0.00        73\n",
      "          26       0.70      0.03      0.06      1008\n",
      "          27       0.00      0.00      0.00        71\n",
      "          28       0.00      0.00      0.00        72\n",
      "          29       0.03      0.00      0.00       608\n",
      "          30       0.00      0.00      0.00       446\n",
      "          31       0.00      0.00      0.00        55\n",
      "          32       0.00      0.00      0.00       259\n",
      "          33       0.00      0.00      0.00       148\n",
      "          34       0.72      0.01      0.01      4497\n",
      "          35       0.00      0.00      0.00        42\n",
      "          36       0.00      0.00      0.00        29\n",
      "          37       0.00      0.00      0.00        65\n",
      "          38       0.00      0.00      0.00       350\n",
      "          39       0.09      0.01      0.02       882\n",
      "          40       0.00      0.00      0.00        76\n",
      "          41       0.17      0.04      0.06       256\n",
      "          42       0.00      0.00      0.00       383\n",
      "          43       0.00      0.00      0.00       522\n",
      "          44       0.00      0.00      0.00       117\n",
      "          45       0.00      0.00      0.00        65\n",
      "          46       0.00      0.00      0.00       997\n",
      "          47       0.27      0.06      0.10       376\n",
      "          48       0.00      0.00      0.00       641\n",
      "          49       0.00      0.00      0.00        27\n",
      "          50       0.00      0.00      0.00       311\n",
      "          51       0.00      0.00      0.00        40\n",
      "          52       0.21      0.11      0.14      2323\n",
      "          53       0.03      0.04      0.04       255\n",
      "          54       0.00      0.00      0.00       275\n",
      "          55       0.00      0.00      0.00      1808\n",
      "          56       0.00      0.00      0.00       742\n",
      "          57       0.00      0.00      0.00        91\n",
      "          58       0.00      0.00      0.00        76\n",
      "          59       0.00      0.00      0.00       302\n",
      "          60       0.00      0.00      0.00       243\n",
      "          61       0.00      0.00      0.00      1056\n",
      "          62       0.00      0.00      0.00      1160\n",
      "          63       0.00      0.00      0.00        37\n",
      "          64       0.00      0.00      0.00       155\n",
      "          65       0.00      0.00      0.00       164\n",
      "          66       0.00      0.00      0.00        31\n",
      "          67       0.00      0.00      0.00       320\n",
      "          68       0.00      0.00      0.00       324\n",
      "          69       0.00      0.00      0.00       143\n",
      "          70       0.08      0.02      0.03       191\n",
      "          71       0.00      0.00      0.00       255\n",
      "          72       0.00      0.00      0.00        33\n",
      "          73       0.00      0.00      0.00        16\n",
      "          74       0.00      0.00      0.00       368\n",
      "          75       0.00      0.00      0.00        54\n",
      "          76       0.00      0.00      0.00       653\n",
      "          77       0.00      0.00      0.00       235\n",
      "          78       0.00      0.00      0.00       152\n",
      "          79       0.00      0.00      0.00       307\n",
      "          80       0.34      0.55      0.42     26146\n",
      "\n",
      "    accuracy                           0.31     79223\n",
      "   macro avg       0.07      0.03      0.03     79223\n",
      "weighted avg       0.27      0.31      0.23     79223\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "actuals, predictions = train_loop(model, epochs=8, lr=0.05, wd=0.00001)\n",
    "print(classification_report(actuals, predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
